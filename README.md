# Aufzugsteuerung mittels Reinforcement Learning

Dieses Projekt implementiert eine Aufzugsteuerung mit Hilfe von Q-Learning. Es simuliert eine realitÃ¤tsnahe Umgebung mit mehreren Etagen, zufÃ¤lligen FahrgÃ¤sten und einer handgebauten Referenzstrategie. Ziel ist es, durch RL eine bessere Steuerungsstrategie zu lernen.

## Projektstruktur

```
project/
â”‚
â”œâ”€â”€ learning.py                # Q-Learning mit g1 und g2
â”œâ”€â”€ reference.py               # Referenzstrategie (klassisch heuristisch)
â”œâ”€â”€ Environment/               # Simulierte Aufzugsumgebung
â”‚   â”œâ”€â”€ environment.py         # ZustÃ¤nde, Aktionen, Step-Funktion
â”‚   â””â”€â”€ constants.py           # Definition von Richtungen, Aktionen, etc.
â”‚   â””â”€â”€ policy.py              # Definition und Auswahl von Strategien
â”œâ”€â”€ comparison_learning_curve.png     # Lernkurvenvergleich g1 vs. g2
â”œâ”€â”€ reference_learning_curve.png      # Lernkurve der Referenzstrategie
â””â”€â”€ README.md                 # Diese Datei
```

## Methodik

Das Projekt nutzt tabellarisches Q-Learning. Die ZustÃ¤nde wurden stark vereinfacht, um die GrÃ¶ÃŸe des Zustandsraums zu reduzieren. Zwei Ein-Schritt-Kostenfunktionen (`g1`, `g2`) wurden definiert, um Belohnungen fÃ¼r verschiedene Verhaltensweisen zu vergeben:

- `g1`: einfache Belohnung fÃ¼r Auslieferung, Schrittstrafe
- `g2`: differenzierte Belohnung (TÃ¼r Ã¶ffnen, Richtungswahl, Vermeidung unnÃ¶tiger Aktionen)

## Referenzstrategie

Die Referenzstrategie (`reference_policy`) modelliert das Verhalten eines klassischen Aufzugs:

- fÃ¤hrt in Richtung nÃ¤chstem Ziel (Cabin- oder Call-Button)
- Ã¶ffnet TÃ¼ren nur wenn nÃ¶tig
- stoppt bei Bedarf unterwegs

Diese Strategie ist deterministisch und nicht lernbasiert. Sie dient als Vergleich fÃ¼r die Performance der gelernten Strategien.

## Training

- 3.000 Episoden mit je 400 Zeitschritten
- Trainingsparameter: `Î± = 0.1`, `Î³ = 0.95`, `Îµ` beginnend bei 0.3 mit linearer Reduktion
- Evaluation Ã¼ber Gesamtreward pro Episode sowie gleitender Durchschnitt

## Ergebnisse

Die Lernkurven zeigen:
- `g2` lernt differenzierteres Verhalten, erfordert aber mehr Episoden
- `g1` ist einfacher zu lernen, aber weniger effektiv
- Die RL-Strategien Ã¼bertreffen die Referenzstrategie langfristig

## Visualisierung

- **`comparison_learning_curve.png`** zeigt die BelohnungsverlÃ¤ufe fÃ¼r `g1` vs. `g2`
- **`reference_learning_curve.png`** zeigt die Performance der Referenzstrategie

## Starten des Trainings

```bash
python learning.py
```

## Evaluierung der Referenzstrategie

```bash
python reference.py
```

## Demonstration

Um eine gelernte Policy in einer Episode zu demonstrieren, verwenden Sie:

```bash
python demonstration.py
```

## ğŸ“„ Bericht / Dokumentation

Der vollstÃ¤ndige Projektbericht mit Methodik, Versuchsaufbau, Lernkurven und Ergebnisanalyse ist hier verfÃ¼gbar:

ğŸ‘‰ [Q-Learning Elevator control (PDF)](https://drive.google.com/file/d/1YxnPScZop35mV5LH4GEtGzOqSD5p0v47/view?usp=share_link)

Der Bericht enthÃ¤lt:
- mathematische Beschreibung des Q-Learning-Algorithmus
- die vereinfachte ZustandsreprÃ¤sentation
- die Definition der Kostenfunktionen `g1` und `g2`
- experimentelle Evaluation mit Hyperparameter-Variation
- Vergleich mit Referenzstrategie

Das Skript verwendet die beste bekannte Policy basierend auf dem Q-Table aus `learning.py` und zeigt exemplarisch den Ablauf einer Episode.
